#!/usr/bin/env python3


import numpy as np
import torch
from scipy.sparse import coo_matrix, csr_matrix, save_npz
from torch_geometric.data import Data
import random
import os
from collections import defaultdict, deque

class SemiSupervisedAwareNeighborSampler:
    """åŠç›‘ç£å­¦ä¹ æ„ŸçŸ¥çš„é‚»å±…é‡‡æ ·å™¨"""
    
    def __init__(self, adj_matrices, labels, sample_sizes=[15, 10, 5]):
        self.adj_matrices = adj_matrices
        self.labels = labels
        self.sample_sizes = sample_sizes
        self.num_hops = len(sample_sizes)
        
        # é¢„å¤„ç†ï¼šæ„å»ºé‚»æ¥åˆ—è¡¨
        self.adj_lists = {}
        for rel_type, adj_matrix in adj_matrices.items():
            adj_list = defaultdict(list)
            row, col = adj_matrix.nonzero()
            for src, dst in zip(row, col):
                adj_list[src].append(dst)
            self.adj_lists[rel_type] = dict(adj_list)
        
        # è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§
        self.node_importance = self._compute_node_importance()
        
    def _compute_node_importance(self):
        """è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§ - ç»“åˆåº¦ä¸­å¿ƒæ€§å’Œæ ‡ç­¾ä¿¡æ¯"""
        num_nodes = len(self.labels)
        
        # 1. åº¦ä¸­å¿ƒæ€§
        total_degree = np.zeros(num_nodes)
        for adj_matrix in self.adj_matrices.values():
            degree = np.array(adj_matrix.sum(axis=1)).flatten()
            total_degree += degree
        
        degree_importance = total_degree / (total_degree.max() + 1e-8)
        
        # 2. æ ‡ç­¾é‡è¦æ€§ - æœ‰æ ‡ç­¾èŠ‚ç‚¹æ›´é‡è¦
        label_importance = np.zeros(num_nodes)
        labeled_mask = self.labels != -1
        label_importance[labeled_mask] = 1.0
        
        # 3. å¤šæ ·æ€§é‡è¦æ€§ - é¿å…è¿‡åº¦é‡‡æ ·æŸä¸€ç±»
        diversity_importance = np.ones(num_nodes)
        if np.any(labeled_mask):
            unique_labels = np.unique(self.labels[labeled_mask])
            for label in unique_labels:
                label_nodes = np.where(self.labels == label)[0]
                # ç±»åˆ«è¶Šå¤§ï¼Œå•ä¸ªèŠ‚ç‚¹é‡è¦æ€§è¶Šä½
                diversity_importance[label_nodes] = 1.0 / np.sqrt(len(label_nodes))
        
        # ç»¼åˆé‡è¦æ€§ï¼š30%åº¦ä¸­å¿ƒæ€§ + 40%æ ‡ç­¾é‡è¦æ€§ + 30%å¤šæ ·æ€§
        importance = (0.3 * degree_importance + 
                     0.4 * label_importance + 
                     0.3 * diversity_importance)
        
        return importance
    
    def label_aware_seed_selection(self, target_seeds=800):
        """æ ‡ç­¾æ„ŸçŸ¥çš„ç§å­é€‰æ‹© - ä¿æŒåŸå§‹åˆ†å¸ƒ"""
        labeled_nodes = np.where(self.labels != -1)[0]
        unique_labels = np.unique(self.labels[labeled_nodes])
        
        seed_nodes = []
        
        # ä¸ºæ¯ä¸ªç±»åˆ«æŒ‰åŸå§‹æ¯”ä¾‹åˆ†é…ç§å­æ•°é‡
        for label in unique_labels:
            label_nodes = labeled_nodes[self.labels[labeled_nodes] == label]
            
            # ä¿æŒåŸå§‹æ¯”ä¾‹
            original_ratio = len(label_nodes) / len(labeled_nodes)
            target_for_label = max(20, int(target_seeds * original_ratio))  # æœ€å°‘20ä¸ª
            target_for_label = min(target_for_label, len(label_nodes))
            
            # åŸºäºé‡è¦æ€§é€‰æ‹©ï¼Œè€Œééšæœº
            label_importance = self.node_importance[label_nodes]
            
            # é‡è¦æ€§é‡‡æ ·ï¼š70%æŒ‰é‡è¦æ€§ï¼Œ30%éšæœº
            num_importance = int(target_for_label * 0.7)
            num_random = target_for_label - num_importance
            
            # é€‰æ‹©é‡è¦æ€§æœ€é«˜çš„èŠ‚ç‚¹
            if num_importance > 0:
                top_indices = np.argsort(label_importance)[-num_importance:]
                seed_nodes.extend(label_nodes[top_indices])
            
            # éšæœºé€‰æ‹©å‰©ä½™èŠ‚ç‚¹
            if num_random > 0:
                remaining_nodes = np.setdiff1d(label_nodes, label_nodes[top_indices] if num_importance > 0 else [])
                if len(remaining_nodes) >= num_random:
                    random_selected = np.random.choice(remaining_nodes, num_random, replace=False)
                    seed_nodes.extend(random_selected)
                else:
                    seed_nodes.extend(remaining_nodes)
            
            print(f"Label {label}: {len(label_nodes)} ä¸ªèŠ‚ç‚¹ -> {len(seed_nodes) - (len(seed_nodes) - target_for_label)} ä¸ªç§å­")
        
        return seed_nodes
    
    def adaptive_neighbor_sampling(self, nodes, rel_type, base_sample_size, hop):
        """è‡ªé€‚åº”é‚»å±…é‡‡æ · - æ ¹æ®èŠ‚ç‚¹ç‰¹æ€§è°ƒæ•´é‡‡æ ·ç­–ç•¥"""
        sampled_neighbors = []
        adj_list = self.adj_lists[rel_type]
        
        for node in nodes:
            if node not in adj_list:
                continue
                
            neighbors = adj_list[node]
            node_degree = len(neighbors)
            
            # è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥
            if node_degree <= 5:
                # ä½åº¦èŠ‚ç‚¹ï¼šä¿ç•™æ‰€æœ‰é‚»å±…
                sample_size = len(neighbors)
            elif node_degree <= 20:
                # ä¸­ç­‰åº¦èŠ‚ç‚¹ï¼šé€‚åº¦é‡‡æ ·
                sample_size = min(base_sample_size, len(neighbors))
            else:
                # é«˜åº¦èŠ‚ç‚¹ï¼šå‡å°‘é‡‡æ ·é¿å…hubåå·®
                sample_size = min(base_sample_size // 2, len(neighbors))
            
            # éšhopé€’å‡
            sample_size = max(1, sample_size // (hop + 1))
            
            if sample_size >= len(neighbors):
                sampled_neighbors.extend(neighbors)
            else:
                # é‡è¦æ€§é‡‡æ ·
                neighbor_importance = self.node_importance[neighbors]
                
                # æ··åˆç­–ç•¥ï¼š50%é‡è¦æ€§é‡‡æ · + 50%éšæœºé‡‡æ ·
                num_importance = sample_size // 2
                num_random = sample_size - num_importance
                
                if num_importance > 0 and len(neighbors) > num_importance:
                    # æŒ‰é‡è¦æ€§é€‰æ‹©
                    top_indices = np.argsort(neighbor_importance)[-num_importance:]
                    sampled_neighbors.extend([neighbors[i] for i in top_indices])
                    
                    # éšæœºé€‰æ‹©å‰©ä½™
                    remaining_neighbors = [neighbors[i] for i in range(len(neighbors)) if i not in top_indices]
                    if len(remaining_neighbors) >= num_random:
                        random_selected = random.sample(remaining_neighbors, num_random)
                        sampled_neighbors.extend(random_selected)
                    else:
                        sampled_neighbors.extend(remaining_neighbors)
                else:
                    # å®Œå…¨éšæœº
                    sampled_neighbors.extend(random.sample(neighbors, sample_size))
        
        return list(set(sampled_neighbors))
    
    def semi_supervised_sampling(self, target_size):
        """åŠç›‘ç£å­¦ä¹ ä¸“ç”¨çš„é‡‡æ ·æ–¹æ³•"""
        
        # 1. æ ‡ç­¾æ„ŸçŸ¥çš„ç§å­é€‰æ‹©
        seed_nodes = self.label_aware_seed_selection(target_seeds=min(1000, target_size//10))
        
        all_sampled_nodes = set(seed_nodes)
        current_layer_nodes = list(seed_nodes)
        
        print(f"[Semi-supervised Sampling] ç§å­èŠ‚ç‚¹: {len(seed_nodes)}")
        print(f"[Semi-supervised Sampling] ç§å­èŠ‚ç‚¹æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(self.labels[seed_nodes][self.labels[seed_nodes] != -1].astype(int))}")
        
        # 2. å¤šè·³è‡ªé€‚åº”é‡‡æ ·
        for hop in range(self.num_hops):
            if len(all_sampled_nodes) >= target_size:
                break
            
            next_layer_nodes = []
            base_sample_size = self.sample_sizes[hop]
            
            print(f"[Hop {hop+1}] å¼€å§‹é‡‡æ ·ï¼Œå½“å‰èŠ‚ç‚¹æ•°: {len(all_sampled_nodes)}")
            
            # å¯¹æ¯ç§å…³ç³»ç±»å‹è¿›è¡Œè‡ªé€‚åº”é‡‡æ ·
            for rel_type in self.adj_matrices.keys():
                neighbors = self.adaptive_neighbor_sampling(
                    current_layer_nodes, rel_type, base_sample_size, hop
                )
                next_layer_nodes.extend(neighbors)
            
            # å»é‡å¹¶ç§»é™¤å·²é‡‡æ ·èŠ‚ç‚¹
            next_layer_nodes = list(set(next_layer_nodes) - all_sampled_nodes)
            
            # å®¹é‡æ§åˆ¶ - åŸºäºé‡è¦æ€§é€‰æ‹©
            remaining_capacity = target_size - len(all_sampled_nodes)
            if remaining_capacity > 0:
                if len(next_layer_nodes) > remaining_capacity:
                    # é‡è¦æ€§é‡‡æ ·
                    node_importance = self.node_importance[next_layer_nodes]
                    importance_probs = node_importance / node_importance.sum()
                    
                    selected_indices = np.random.choice(
                        len(next_layer_nodes), 
                        remaining_capacity, 
                        replace=False, 
                        p=importance_probs
                    )
                    next_layer_nodes = [next_layer_nodes[i] for i in selected_indices]
                
                all_sampled_nodes.update(next_layer_nodes)
                current_layer_nodes = next_layer_nodes
                
                print(f"[Hop {hop+1}] é‡‡æ ·é‚»å±…: {len(next_layer_nodes)}, ç´¯è®¡: {len(all_sampled_nodes)}")
            else:
                break
        
        # 3. å›¾è¿é€šæ€§ä¿®å¤
        final_nodes = list(all_sampled_nodes)
        final_nodes = self._repair_graph_connectivity(final_nodes, target_size)
        
        return final_nodes
    
    def _repair_graph_connectivity(self, nodes, target_size):
        """ä¿®å¤å›¾è¿é€šæ€§ - ç¡®ä¿æ ‡ç­¾ä¼ æ’­è·¯å¾„"""
        if len(nodes) >= target_size:
            return nodes[:target_size]
        
        current_nodes = set(nodes)
        labeled_nodes = [n for n in nodes if self.labels[n] != -1]
        unlabeled_nodes = [n for n in nodes if self.labels[n] == -1]
        
        print(f"[è¿é€šæ€§ä¿®å¤] å½“å‰: {len(nodes)} èŠ‚ç‚¹, æ ‡ç­¾: {len(labeled_nodes)}, æ— æ ‡ç­¾: {len(unlabeled_nodes)}")
        
        # ç¡®ä¿æ¯ä¸ªæ— æ ‡ç­¾èŠ‚ç‚¹éƒ½æœ‰åˆ°æ ‡ç­¾èŠ‚ç‚¹çš„è·¯å¾„
        bridge_nodes = set()
        
        for unlabeled_node in unlabeled_nodes:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¡¥æ¥èŠ‚ç‚¹
            has_labeled_neighbor = False
            
            for rel_type, adj_list in self.adj_lists.items():
                if unlabeled_node in adj_list:
                    neighbors = adj_list[unlabeled_node]
                    for neighbor in neighbors:
                        if neighbor in labeled_nodes:
                            has_labeled_neighbor = True
                            break
                    if has_labeled_neighbor:
                        break
            
            # å¦‚æœæ²¡æœ‰ç›´æ¥è¿æ¥åˆ°æ ‡ç­¾èŠ‚ç‚¹ï¼Œå¯»æ‰¾æ¡¥æ¥èŠ‚ç‚¹
            if not has_labeled_neighbor:
                for rel_type, adj_list in self.adj_lists.items():
                    if unlabeled_node in adj_list:
                        neighbors = adj_list[unlabeled_node]
                        for neighbor in neighbors:
                            if neighbor not in current_nodes and neighbor < len(self.labels):
                                # æ£€æŸ¥è¿™ä¸ªé‚»å±…æ˜¯å¦è¿æ¥åˆ°æ ‡ç­¾èŠ‚ç‚¹
                                for rel_type2, adj_list2 in self.adj_lists.items():
                                    if neighbor in adj_list2:
                                        neighbor_neighbors = adj_list2[neighbor]
                                        if any(nn in labeled_nodes for nn in neighbor_neighbors):
                                            bridge_nodes.add(neighbor)
                                            break
        
        # æ·»åŠ æ¡¥æ¥èŠ‚ç‚¹
        remaining_capacity = target_size - len(current_nodes)
        bridge_nodes = list(bridge_nodes)[:remaining_capacity]
        
        final_nodes = list(current_nodes) + bridge_nodes
        
        print(f"[è¿é€šæ€§ä¿®å¤] æ·»åŠ  {len(bridge_nodes)} ä¸ªæ¡¥æ¥èŠ‚ç‚¹")
        
        return final_nodes


def create_edge_type_from_attr(edge_attr):
    """
    åŸºäºedge_attrï¼ˆé‡‘é¢ã€æ—¶é—´å·®ï¼‰è‡ªåŠ¨åˆ’åˆ†4ç±»å…³ç³»
    edge_attr: tensor of shape [num_edges, 2], å…¶ä¸­ç¬¬0åˆ—æ˜¯é‡‘é¢ï¼Œç¬¬1åˆ—æ˜¯æ—¶é—´å·®
    
    è¿”å›:
    - edge_types: numpy arrayï¼Œæ¯æ¡è¾¹çš„å…³ç³»ç±»å‹ (0-3)
    - relation_info: dictï¼Œè®°å½•æ¯ç§å…³ç³»çš„å«ä¹‰
    """
    
    # å‡è®¾edge_attr[:, 0]æ˜¯é‡‘é¢ï¼Œedge_attr[:, 1]æ˜¯æ—¶é—´å·®
    amounts = edge_attr[:, 0].numpy() if hasattr(edge_attr, 'numpy') else edge_attr[:, 0]
    time_deltas = edge_attr[:, 1].numpy() if hasattr(edge_attr, 'numpy') else edge_attr[:, 1]
    
    print(f"é‡‘é¢èŒƒå›´: [{amounts.min():.2f}, {amounts.max():.2f}]")
    print(f"æ—¶é—´å·®èŒƒå›´: [{time_deltas.min():.2f}, {time_deltas.max():.2f}]")
    
    # ä½¿ç”¨ä¸­ä½æ•°è¿›è¡ŒäºŒå€¼åˆ’åˆ†
    amount_median = np.median(amounts)
    time_median = np.median(time_deltas)
    
    print(f"é‡‘é¢ä¸­ä½æ•°: {amount_median:.2f}")
    print(f"æ—¶é—´å·®ä¸­ä½æ•°: {time_median:.2f}")
    
    # åˆ›å»ºäºŒè¿›åˆ¶æ ‡è®°
    amount_high = amounts >= amount_median  # True=é«˜é¢, False=ä½é¢
    time_high = time_deltas >= time_median   # True=é«˜é¢‘(æ—¶é—´å·®å¤§), False=ä½é¢‘
    
    # ç»„åˆæˆ4ç±»å…³ç³»
    edge_types = np.zeros(len(amounts), dtype=int)
    
    # Rel-0: low-amount & low-freq (ä½é¢-ä½é¢‘)
    mask_0 = (~amount_high) & (~time_high)
    edge_types[mask_0] = 0
    
    # Rel-1: low-amount & high-freq (ä½é¢-é«˜é¢‘) 
    mask_1 = (~amount_high) & time_high
    edge_types[mask_1] = 1
    
    # Rel-2: high-amount & low-freq (é«˜é¢-ä½é¢‘)
    mask_2 = amount_high & (~time_high)
    edge_types[mask_2] = 2
    
    # Rel-3: high-amount & high-freq (é«˜é¢-é«˜é¢‘)
    mask_3 = amount_high & time_high
    edge_types[mask_3] = 3
    
    # ç»Ÿè®¡æ¯ç§å…³ç³»çš„è¾¹æ•°
    relation_info = {}
    for i in range(4):
        count = np.sum(edge_types == i)
        relation_info[i] = {
            'count': count,
            'percentage': count / len(edge_types) * 100,
            'description': [
                'low-amount & low-freq (ä½é¢-ä½é¢‘)',
                'low-amount & high-freq (ä½é¢-é«˜é¢‘)', 
                'high-amount & low-freq (é«˜é¢-ä½é¢‘)',
                'high-amount & high-freq (é«˜é¢-é«˜é¢‘)'
            ][i]
        }
        print(f"Rel-{i}: {relation_info[i]['description']} - {count:,} è¾¹ ({relation_info[i]['percentage']:.1f}%)")
    
    return edge_types, relation_info


def create_fixed_dataset(original_data_path="data.pt", output_dir="fixed_semisup_dataset"):
    """åˆ›å»ºä¿®å¤åçš„æ•°æ®é›†"""
    
    print("=" * 60)
    print("åˆ›å»ºåŠç›‘ç£å­¦ä¹ ä¼˜åŒ–çš„æ•°æ®é›†ï¼ˆåŸºäºedge_attrè‡ªåŠ¨åˆ’åˆ†å…³ç³»ï¼‰")
    print("=" * 60)
    
    # 1. åŠ è½½åŸå§‹æ•°æ®
    data = torch.load(original_data_path, weights_only=False)
    edge_index = data['edge_index']
    num_nodes = data['num_nodes'] if 'num_nodes' in data else int(edge_index.max().item()) + 1
    
    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    features = data['X'].numpy()
    if 'labels' in data:
        labels = data['labels'].numpy()
    elif hasattr(data, 'y'):
        labels = data.y.numpy()
    else:
        raise KeyError("æœªæ‰¾åˆ°æ ‡ç­¾æ•°æ®")
    
    print(f"åŸå§‹æ•°æ®: {num_nodes} èŠ‚ç‚¹, {features.shape[1]} ç‰¹å¾ç»´åº¦")
    
    # 2. æ£€æŸ¥å¹¶å¤„ç†edge_attr
    if 'edge_attr' not in data or data['edge_attr'] is None:
        print("âš ï¸  æœªæ‰¾åˆ°edge_attrï¼Œå°†ç”Ÿæˆæ¨¡æ‹Ÿçš„é‡‘é¢å’Œæ—¶é—´å·®æ•°æ®")
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„edge_attr: [é‡‘é¢, æ—¶é—´å·®]
        num_edges = edge_index.shape[1]
        # é‡‘é¢ï¼šå¯¹æ•°æ­£æ€åˆ†å¸ƒæ¨¡æ‹ŸçœŸå®äº¤æ˜“
        amounts = np.random.lognormal(mean=5, sigma=2, size=num_edges)
        # æ—¶é—´å·®ï¼šæŒ‡æ•°åˆ†å¸ƒæ¨¡æ‹Ÿäº¤æ˜“é—´éš”
        time_deltas = np.random.exponential(scale=10, size=num_edges)
        edge_attr = torch.tensor(np.column_stack([amounts, time_deltas]), dtype=torch.float32)
        print(f"ç”Ÿæˆæ¨¡æ‹Ÿedge_attr: {edge_attr.shape}")
    else:
        edge_attr = data['edge_attr']
        print(f"æ‰¾åˆ°edge_attr: {edge_attr.shape}")
        
        # ç¡®ä¿edge_attrè‡³å°‘æœ‰2åˆ—ï¼ˆé‡‘é¢ã€æ—¶é—´ï¼‰
        if edge_attr.shape[1] < 2:
            print("âš ï¸  edge_attråˆ—æ•°ä¸è¶³ï¼Œè¡¥å……æ—¶é—´å·®åˆ—")
            # å¦‚æœåªæœ‰1åˆ—ï¼Œå‡è®¾æ˜¯é‡‘é¢ï¼Œè¡¥å……æ—¶é—´å·®
            time_deltas = np.random.exponential(scale=10, size=edge_attr.shape[0])
            edge_attr = torch.cat([edge_attr, torch.tensor(time_deltas).unsqueeze(1)], dim=1)
    
    # 3. åŸºäºedge_attrè‡ªåŠ¨åˆ’åˆ†å…³ç³»ç±»å‹
    print(f"\nåŸºäºedge_attrè‡ªåŠ¨åˆ’åˆ†å…³ç³»ç±»å‹...")
    edge_types, relation_info = create_edge_type_from_attr(edge_attr)
    
    # 4. æ„å»ºå¤šå…³ç³»é‚»æ¥çŸ©é˜µ
    src = edge_index[0].numpy()
    dst = edge_index[1].numpy()
    
    adj_matrices = {}
    
    for rel_type in range(4):  # å›ºå®š4ç§å…³ç³»
        mask = edge_types == rel_type
        if not np.any(mask):
            print(f"âš ï¸  å…³ç³»ç±»å‹ {rel_type} æ²¡æœ‰è¾¹ï¼Œè·³è¿‡")
            continue
            
        src_rel = src[mask]
        dst_rel = dst[mask]
        adj_rel = coo_matrix((np.ones(len(src_rel)), (src_rel, dst_rel)), 
                            shape=(num_nodes, num_nodes))
        adj_matrices[rel_type] = adj_rel
        print(f"å…³ç³» {rel_type} ({relation_info[rel_type]['description']}): {adj_rel.nnz:,} æ¡è¾¹")
    
    # 5. ä½¿ç”¨æ”¹è¿›çš„é‡‡æ ·å™¨
    TARGET_NODES = 15000
    sampler = SemiSupervisedAwareNeighborSampler(
        adj_matrices, labels, sample_sizes=[20, 15, 10]
    )
    
    print(f"\nå¼€å§‹åŠç›‘ç£ä¼˜åŒ–é‡‡æ ·...")
    selected_nodes = sampler.semi_supervised_sampling(TARGET_NODES)
    
    print(f"æœ€ç»ˆé‡‡æ ·èŠ‚ç‚¹æ•°: {len(selected_nodes)}")
    
    # 6. æ„å»ºå­å›¾
    node_id_map = {old: new for new, old in enumerate(selected_nodes)}
    sub_features = features[selected_nodes]
    sub_labels = labels[selected_nodes]
    
    # åˆ†ææ ‡ç­¾åˆ†å¸ƒä¿æŒæƒ…å†µ
    original_labeled = labels[labels != -1]
    sub_labeled = sub_labels[sub_labels != -1]
    
    correlation = 0
    if len(original_labeled) > 0 and len(sub_labeled) > 0:
        original_dist = np.bincount(original_labeled.astype(int))
        sub_dist = np.bincount(sub_labeled.astype(int))
        
        # è®¡ç®—åˆ†å¸ƒç›¸ä¼¼æ€§
        min_len = min(len(original_dist), len(sub_dist))
        if min_len > 1:
            correlation = np.corrcoef(
                original_dist[:min_len] / original_dist[:min_len].sum(),
                sub_dist[:min_len] / sub_dist[:min_len].sum()
            )[0, 1]
            print(f"ç±»åˆ«åˆ†å¸ƒä¿æŒç›¸ä¼¼æ€§: {correlation:.4f}")
    
    # 7. æ„å»ºå­å›¾é‚»æ¥çŸ©é˜µ
    sub_adj_matrices = {}
    total_edges = 0
    
    for rel_type, adj_matrix in adj_matrices.items():
        row, col = adj_matrix.nonzero()
        
        # ç­›é€‰å­å›¾è¾¹
        mask = np.isin(row, selected_nodes) & np.isin(col, selected_nodes)
        src_sub = [node_id_map[s] for s in row[mask]]
        dst_sub = [node_id_map[d] for d in col[mask]]
        
        adj_sub = coo_matrix((np.ones(len(src_sub)), (src_sub, dst_sub)),
                            shape=(len(selected_nodes), len(selected_nodes)))
        sub_adj_matrices[rel_type] = adj_sub
        total_edges += adj_sub.nnz
        
        print(f"å­å›¾å…³ç³» {rel_type} ({relation_info[rel_type]['description']}): {adj_sub.nnz:,} æ¡è¾¹")
    
    # 8. åˆ†æå›¾è´¨é‡
    analyze_graph_quality_for_semisupervised(sub_adj_matrices, sub_labels, len(selected_nodes))
    
    # 9. ä¿å­˜æ•°æ®é›†
    os.makedirs(output_dir, exist_ok=True)
    
    for rel_type, adj_sub in sub_adj_matrices.items():
        save_npz(os.path.join(output_dir, f"adj_relation{rel_type}.npz"), adj_sub)
    
    save_npz(os.path.join(output_dir, "features.npz"), csr_matrix(sub_features))
    np.save(os.path.join(output_dir, "labels.npy"), sub_labels)
    
    # ä¿å­˜å…³ç³»ä¿¡æ¯å’Œæ”¹è¿›ä¿¡æ¯
    improvement_info = {
        'sampling_method': 'semi_supervised_aware_neighbor_sampling',
        'relation_method': 'edge_attr_based_4_relations',
        'relation_info': relation_info,
        'improvements': [
            'label_aware_seed_selection',
            'adaptive_neighbor_sampling',
            'importance_based_selection',
            'graph_connectivity_repair',
            'edge_attr_based_relation_types'
        ],
        'target_nodes': TARGET_NODES,
        'actual_nodes': len(selected_nodes),
        'distribution_correlation': correlation,
        'total_edges': total_edges
    }
    
    np.save(os.path.join(output_dir, "improvement_info.npy"), improvement_info)
    
    print(f"\n{'='*60}")
    print(f"åŠç›‘ç£ä¼˜åŒ–æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_dir}/")
    print(f"{'='*60}")
    print(f"ğŸ”§ ä¸»è¦æ”¹è¿›:")
    print(f"  âœ… åŸºäºedge_attrè‡ªåŠ¨åˆ’åˆ†4ç±»å…³ç³»ï¼ˆé‡‘é¢Ã—æ—¶é—´ï¼‰")
    print(f"  âœ… æ ‡ç­¾æ„ŸçŸ¥çš„ç§å­é€‰æ‹©ï¼ˆä¿æŒåŸå§‹åˆ†å¸ƒï¼‰")
    print(f"  âœ… è‡ªé€‚åº”é‚»å±…é‡‡æ ·ï¼ˆé¿å…hubåå·®ï¼‰")
    print(f"  âœ… é‡è¦æ€§å¼•å¯¼çš„èŠ‚ç‚¹é€‰æ‹©")
    print(f"  âœ… å›¾è¿é€šæ€§ä¿®å¤ï¼ˆç¡®ä¿æ ‡ç­¾ä¼ æ’­è·¯å¾„ï¼‰")
    print(f"\nğŸ“Š å…³ç³»ç±»å‹åˆ†å¸ƒ:")
    for rel_type, info in relation_info.items():
        print(f"  - Rel-{rel_type}: {info['description']} ({info['percentage']:.1f}%)")
    print(f"\nğŸ“Š æ•°æ®é›†è´¨é‡:")
    print(f"  - èŠ‚ç‚¹æ•°: {len(selected_nodes):,}")
    print(f"  - è¾¹æ•°: {total_edges:,}")
    print(f"  - ç±»åˆ«åˆ†å¸ƒç›¸ä¼¼æ€§: {correlation:.4f}" if correlation > 0 else "  - ç±»åˆ«åˆ†å¸ƒç›¸ä¼¼æ€§: N/A")
    
    return output_dir


def analyze_graph_quality_for_semisupervised(adj_matrices, labels, num_nodes):
    """åˆ†æå›¾è´¨é‡å¯¹åŠç›‘ç£å­¦ä¹ çš„é€‚ç”¨æ€§"""
    
    print(f"\nå›¾è´¨é‡åˆ†æï¼ˆåŠç›‘ç£å­¦ä¹ è§†è§’ï¼‰:")
    print("-" * 40)
    
    # 1. æ ‡ç­¾ä¼ æ’­è·¯å¾„åˆ†æ
    labeled_nodes = np.where(labels != -1)[0]
    unlabeled_nodes = np.where(labels == -1)[0]
    
    print(f"æœ‰æ ‡ç­¾èŠ‚ç‚¹: {len(labeled_nodes)} ({len(labeled_nodes)/num_nodes*100:.1f}%)")
    print(f"æ— æ ‡ç­¾èŠ‚ç‚¹: {len(unlabeled_nodes)} ({len(unlabeled_nodes)/num_nodes*100:.1f}%)")
    
    # 2. è¿é€šæ€§åˆ†æ
    # åˆå¹¶æ‰€æœ‰å…³ç³»çš„é‚»æ¥çŸ©é˜µ
    combined_adj = None
    for adj in adj_matrices.values():
        if combined_adj is None:
            combined_adj = adj
        else:
            combined_adj = combined_adj + adj
    
    # ç®€å•çš„è¿é€šæ€§æ£€æŸ¥
    row, col = combined_adj.nonzero()
    edge_count = len(row)
    density = edge_count / (num_nodes * num_nodes)
    avg_degree = edge_count * 2 / num_nodes
    
    print(f"å›¾å¯†åº¦: {density:.6f}")
    print(f"å¹³å‡åº¦æ•°: {avg_degree:.2f}")
    
    # 3. æ ‡ç­¾-æ— æ ‡ç­¾è¿æ¥åˆ†æ
    labeled_to_unlabeled_edges = 0
    unlabeled_to_unlabeled_edges = 0
    
    for src, dst in zip(row, col):
        if src in labeled_nodes and dst in unlabeled_nodes:
            labeled_to_unlabeled_edges += 1
        elif src in unlabeled_nodes and dst in unlabeled_nodes:
            unlabeled_to_unlabeled_edges += 1
    
    if len(unlabeled_nodes) > 0:
        labeled_unlabeled_ratio = labeled_to_unlabeled_edges / len(unlabeled_nodes)
        print(f"æ ‡ç­¾-æ— æ ‡ç­¾è¿æ¥æ¯”ä¾‹: {labeled_unlabeled_ratio:.2f} (æ¯ä¸ªæ— æ ‡ç­¾èŠ‚ç‚¹å¹³å‡è¿æ¥åˆ°æ ‡ç­¾èŠ‚ç‚¹çš„è¾¹æ•°)")
        
        if labeled_unlabeled_ratio < 0.5:
            print("âš ï¸  è­¦å‘Š: æ ‡ç­¾-æ— æ ‡ç­¾è¿æ¥è¿‡å°‘ï¼Œå¯èƒ½å½±å“æ ‡ç­¾ä¼ æ’­æ•ˆæœ")
        else:
            print("âœ… æ ‡ç­¾-æ— æ ‡ç­¾è¿æ¥å……è¶³ï¼Œæœ‰åˆ©äºåŠç›‘ç£å­¦ä¹ ")


def main():
    """ä¸»å‡½æ•°"""
    
    print("å¼€å§‹åˆ›å»ºåŠç›‘ç£å­¦ä¹ ä¼˜åŒ–çš„æ•°æ®é›†...")
    
    try:
        # åˆ›å»ºä¿®å¤åçš„æ•°æ®é›†
        output_dir = create_fixed_dataset()
        
        print(f"\nğŸ‰ æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
        print(f"ğŸ“ ä½ç½®: {output_dir}")
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print(f"1. ç”¨è¿™ä¸ªæ–°æ•°æ®é›†é‡æ–°è®­ç»ƒEnhanced CARE-GNN")
        print(f"2. å¯¹æ¯”åŸå§‹æ•°æ®é›†vsä¿®å¤æ•°æ®é›†çš„åŠç›‘ç£å­¦ä¹ æ•ˆæœ")
        print(f"3. æ£€æŸ¥alphaå‚æ•°æœç´¢ç»“æœæ˜¯å¦æœ‰æ”¹å–„")
        print(f"4. è§‚å¯Ÿæ³¨æ„åŠ›æƒé‡æ˜¯å¦æ›´åŠ ç¨³å®š")
        
        print(f"\nğŸ”¬ é¢„æœŸæ”¹è¿›:")
        print(f"- 4ç§å…³ç³»ç±»å‹åŸºäºçœŸå®äº¤æ˜“ç‰¹å¾ï¼ˆé‡‘é¢Ã—æ—¶é—´ï¼‰æ›´æœ‰æ„ä¹‰")
        print(f"- åŠç›‘ç£å­¦ä¹ æ•ˆæœåº”è¯¥æ˜¾è‘—æå‡")
        print(f"- æ³¨æ„åŠ›æƒé‡æ•°å€¼åº”è¯¥æ›´ç¨³å®š")  
        print(f"- ä¸åŒalphaå€¼ä¹‹é—´çš„æ€§èƒ½å·®å¼‚åº”è¯¥æ›´æ˜æ˜¾")
        print(f"- å›¾æŸå¤±åº”è¯¥å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ­£å‘è´¡çŒ®")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":

    main()
