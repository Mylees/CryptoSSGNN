# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆä¼ ç»Ÿæœºå™¨å­¦ä¹ å¯¹æ¯”å®éªŒ
è§£å†³å¯¼å…¥é—®é¢˜çš„æœ€ç®€ç‰ˆæœ¬
"""

import numpy as np
import os
import sys
import time
import warnings
warnings.filterwarnings('ignore')

# ç›´æ¥å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“
from scipy.sparse import load_npz, csr_matrix
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

print("ğŸ“š Libraries imported successfully!")

def load_and_preprocess_data(data_dir):
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    print(f"ğŸ“ Loading data from: {data_dir}")
    
    # åŠ è½½ç‰¹å¾å’Œæ ‡ç­¾
    features = load_npz(os.path.join(data_dir, 'features.npz')).toarray()
    labels = np.load(os.path.join(data_dir, 'labels.npy')).astype(np.int64)
    
    print(f"âœ… Features shape: {features.shape}")
    print(f"âœ… Labels shape: {labels.shape}")
    print(f"âœ… Unique labels: {np.unique(labels)}")
    
    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    valid_mask = labels >= 0
    X = features[valid_mask]
    y = labels[valid_mask]
    
    print(f"âœ… Valid samples: {len(X)}")
    print(f"âœ… Class distribution: {np.bincount(y)}")
    
    return X, y

def run_ml_models(X, y):
    """è¿è¡Œæœºå™¨å­¦ä¹ æ¨¡å‹"""
    print("\nğŸ¤– Starting ML model comparison...")
    
    # åˆ’åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"âœ… Train: {len(X_train)}, Test: {len(X_test)}")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # å®šä¹‰æ¨¡å‹
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'SVM': SVC(random_state=42, probability=True),
        'K-Neighbors': KNeighborsClassifier(n_neighbors=5),
        'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),
        'Naive Bayes': GaussianNB(),
        'MLP': MLPClassifier(hidden_layer_sizes=(64, 32), random_state=42, max_iter=300)
    }
    
    results = {}
    
    print("\n" + "="*60)
    for name, model in models.items():
        print(f"ğŸ”„ Training {name}...")
        
        try:
            start_time = time.time()
            
            # è®­ç»ƒ
            model.fit(X_train_scaled, y_train)
            
            # é¢„æµ‹
            y_pred = model.predict(X_test_scaled)
            
            # è·å–æ¦‚ç‡é¢„æµ‹ (ç”¨äºAUCè®¡ç®—)
            if hasattr(model, 'predict_proba'):
                y_prob = model.predict_proba(X_test_scaled)[:, 1]
            elif hasattr(model, 'decision_function'):
                decision_scores = model.decision_function(X_test_scaled)
                # å°†å†³ç­–åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡ (sigmoidå‡½æ•°)
                y_prob = 1 / (1 + np.exp(-decision_scores))
            else:
                y_prob = y_pred.astype(float)
            
            # è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='macro', zero_division=0)
            recall = recall_score(y_test, y_pred, average='macro', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)
            
            # è®¡ç®—AUC
            try:
                if len(np.unique(y_test)) > 1:
                    auc = roc_auc_score(y_test, y_prob)
                else:
                    auc = 0.0
            except Exception as e:
                print(f"   Warning: AUC calculation failed for {name}: {e}")
                auc = 0.0
            
            training_time = time.time() - start_time
            
            # å­˜å‚¨å®Œæ•´æŒ‡æ ‡
            results[name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc,
                'time': training_time
            }
            
            print(f"âœ… {name} completed:")
            print(f"   Accuracy: {accuracy:.4f}")
            print(f"   Precision: {precision:.4f}")
            print(f"   Recall: {recall:.4f}")
            print(f"   F1: {f1:.4f}")
            print(f"   AUC: {auc:.4f}")
            print(f"   Time: {training_time:.2f}s")
            
        except Exception as e:
            print(f"âŒ {name} failed: {str(e)}")
            results[name] = {
                'accuracy': 0, 'precision': 0, 'recall': 0, 
                'f1': 0, 'auc': 0, 'time': 0, 'error': str(e)
            }
    
    return results

def print_summary(results):
    """æ‰“å°å®Œæ•´çš„ç»“æœæ±‡æ€»"""
    print("\n" + "="*80)
    print("ğŸ“Š COMPREHENSIVE EVALUATION RESULTS")
    print("="*80)
    
    # è¿‡æ»¤æ‰æœ‰é”™è¯¯çš„ç»“æœ
    valid_results = {k: v for k, v in results.items() if 'error' not in v}
    
    if not valid_results:
        print("âŒ No valid results to display!")
        return
    
    # æŒ‰F1åˆ†æ•°æ’åº
    sorted_results = sorted(valid_results.items(), key=lambda x: x[1]['f1'], reverse=True)
    
    # è¯¦ç»†ç»“æœè¡¨æ ¼
    print(f"{'Rank':<4} {'Model':<18} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'AUC':<10} {'Time(s)':<8}")
    print("-" * 82)
    
    for rank, (name, metrics) in enumerate(sorted_results, 1):
        print(f"{rank:<4} {name:<18} {metrics['accuracy']:<10.4f} {metrics['precision']:<10.4f} "
              f"{metrics['recall']:<10.4f} {metrics['f1']:<10.4f} {metrics['auc']:<10.4f} {metrics['time']:<8.2f}")
    
    # æœ€ä½³æ¨¡å‹è¯¦ç»†ä¿¡æ¯
    best_model = sorted_results[0]
    print(f"\nğŸ† BEST MODEL: {best_model[0]}")
    print("="*50)
    print(f"ğŸ“ˆ Accuracy:  {best_model[1]['accuracy']:.4f} ({best_model[1]['accuracy']*100:.2f}%)")
    print(f"ğŸ¯ Precision: {best_model[1]['precision']:.4f}")
    print(f"ğŸ” Recall:    {best_model[1]['recall']:.4f}")
    print(f"âš–ï¸  F1 Score:  {best_model[1]['f1']:.4f}")
    print(f"ğŸ“Š AUC:       {best_model[1]['auc']:.4f}")
    print(f"â±ï¸  Time:      {best_model[1]['time']:.2f} seconds")
    
    # æ€§èƒ½ç»Ÿè®¡
    print(f"\nğŸ“Š PERFORMANCE STATISTICS")
    print("="*50)
    
    all_accuracies = [metrics['accuracy'] for metrics in valid_results.values()]
    all_f1s = [metrics['f1'] for metrics in valid_results.values()]
    all_aucs = [metrics['auc'] for metrics in valid_results.values()]
    
    print(f"Average Accuracy: {np.mean(all_accuracies):.4f} Â± {np.std(all_accuracies):.4f}")
    print(f"Average F1 Score: {np.mean(all_f1s):.4f} Â± {np.std(all_f1s):.4f}")
    print(f"Average AUC:      {np.mean(all_aucs):.4f} Â± {np.std(all_aucs):.4f}")
    print(f"Best Accuracy:    {np.max(all_accuracies):.4f}")
    print(f"Best F1 Score:    {np.max(all_f1s):.4f}")
    print(f"Best AUC:         {np.max(all_aucs):.4f}")
    
    # æ¨¡å‹æ¯”è¾ƒæ´å¯Ÿ
    print(f"\nğŸ” MODEL COMPARISON INSIGHTS")
    print("="*50)
    
    # æ‰¾å‡ºåœ¨ä¸åŒæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³çš„æ¨¡å‹
    best_acc_model = max(valid_results.items(), key=lambda x: x[1]['accuracy'])
    best_f1_model = max(valid_results.items(), key=lambda x: x[1]['f1'])
    best_auc_model = max(valid_results.items(), key=lambda x: x[1]['auc'])
    fastest_model = min(valid_results.items(), key=lambda x: x[1]['time'])
    
    print(f"Best Accuracy:  {best_acc_model[0]} ({best_acc_model[1]['accuracy']:.4f})")
    print(f"Best F1 Score:  {best_f1_model[0]} ({best_f1_model[1]['f1']:.4f})")
    print(f"Best AUC:       {best_auc_model[0]} ({best_auc_model[1]['auc']:.4f})")
    print(f"Fastest Model:  {fastest_model[0]} ({fastest_model[1]['time']:.2f}s)")
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
    try:
        import json
        os.makedirs('ml_results', exist_ok=True)
        
        # åˆ›å»ºè¯¦ç»†çš„ç»“æœå­—å…¸
        detailed_results = {
            'summary': {
                'best_model': best_model[0],
                'best_f1': float(best_model[1]['f1']),
                'num_models_tested': len(valid_results),
                'average_accuracy': float(np.mean(all_accuracies)),
                'average_f1': float(np.mean(all_f1s)),
                'average_auc': float(np.mean(all_aucs))
            },
            'detailed_results': {}
        }
        
        for model_name, metrics in valid_results.items():
            detailed_results['detailed_results'][model_name] = {
                'accuracy': float(metrics['accuracy']),
                'precision': float(metrics['precision']),
                'recall': float(metrics['recall']),
                'f1': float(metrics['f1']),
                'auc': float(metrics['auc']),
                'training_time': float(metrics['time'])
            }
        
        with open('ml_results/comprehensive_results.json', 'w') as f:
            json.dump(detailed_results, f, indent=2)
        
        print(f"\nğŸ’¾ Detailed results saved to: ml_results/comprehensive_results.json")
        
    except Exception as e:
        print(f"âŒ Could not save results: {e}")
    
    # é”™è¯¯æŠ¥å‘Šï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    error_results = {k: v for k, v in results.items() if 'error' in v}
    if error_results:
        print(f"\nâš ï¸  MODELS WITH ERRORS")
        print("="*50)
        for model_name, result in error_results.items():
            print(f"âŒ {model_name}: {result['error']}")
        

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Simple ML Baseline Comparison")
    print("="*50)
    
    # ä¿®å¤å‘½ä»¤è¡Œå‚æ•°è§£æ
    data_dir = "small_multirel_dataset"  # é»˜è®¤å€¼
    
    # ç®€å•çš„å‚æ•°è§£æ
    if len(sys.argv) > 1:
        for i, arg in enumerate(sys.argv):
            if arg == "--data_dir" and i + 1 < len(sys.argv):
                data_dir = sys.argv[i + 1]
                break
            elif not arg.startswith("--") and arg != sys.argv[0]:
                # å¦‚æœæ˜¯éé€‰é¡¹å‚æ•°ï¼Œå°±å½“ä½œæ•°æ®ç›®å½•
                data_dir = arg
                break
    
    print(f"ğŸ“ Using data directory: {data_dir}")
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_dir):
        print(f"âŒ Directory '{data_dir}' does not exist!")
        print("Available directories:")
        for item in os.listdir('.'):
            if os.path.isdir(item):
                print(f"  - {item}")
        return
    
    try:
        # åŠ è½½æ•°æ®
        X, y = load_and_preprocess_data(data_dir)
        
        # è¿è¡Œæ¨¡å‹
        results = run_ml_models(X, y)
        
        # æ‰“å°ç»“æœ
        print_summary(results)
        
        print(f"\nâœ… Comparison completed successfully!")
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()