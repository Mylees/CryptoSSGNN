import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
import argparse
import os
import time
import warnings
import json
from scipy.sparse import load_npz, csr_matrix
warnings.filterwarnings('ignore')

# å¯¼å…¥æ‚¨çš„åŸå§‹æ¨¡å—
from CryptoSSGNN0808 import (
    load_data, sparse_to_edge_index, construct_attribute_graph,
    evaluate_model, set_seed, EnhancedCAREGNN
)

class SemiSupervisedMixin:
    """åŠç›‘ç£å­¦ä¹ æ··å…¥ç±» - ä¸ºæ‰€æœ‰åŸºçº¿æ¨¡å‹æä¾›åŠç›‘ç£èƒ½åŠ›"""
    
    def compute_contrastive_loss(self, embeddings, edge_indices, temperature=0.1, num_neg_samples=3):
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤± - ç»Ÿä¸€çš„æ— ç›‘ç£æŸå¤±å‡½æ•°
        Args:
            embeddings: [num_nodes, embed_dim] èŠ‚ç‚¹åµŒå…¥
            edge_indices: list of [2, num_edges] è¾¹ç´¢å¼•åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            num_neg_samples: è´Ÿæ ·æœ¬æ•°é‡
        """
        device = embeddings.device
        total_loss = 0
        num_valid_relations = 0
        
        for edge_index in edge_indices:
            if edge_index.size(1) == 0:
                continue
                
            num_valid_relations += 1
            
            # é™åˆ¶è¾¹æ•°é‡ï¼Œé˜²æ­¢è®¡ç®—çˆ†ç‚¸
            max_edges = min(1000, edge_index.size(1))
            if edge_index.size(1) > max_edges:
                perm = torch.randperm(edge_index.size(1), device=device)[:max_edges]
                edge_index = edge_index[:, perm]
            
            src, dst = edge_index
            
            # æ ‡å‡†åŒ–åµŒå…¥
            embeddings_norm = F.normalize(embeddings, p=2, dim=1)
            
            # æ­£æ ·æœ¬åˆ†æ•°
            pos_scores = (embeddings_norm[src] * embeddings_norm[dst]).sum(dim=1) / temperature
            
            # è´Ÿé‡‡æ ·
            num_nodes = embeddings.size(0)
            neg_dst = torch.randint(0, num_nodes, (src.size(0), num_neg_samples), device=device)
            
            # ç¡®ä¿è´Ÿæ ·æœ¬ä¸ç­‰äºæ­£æ ·æœ¬
            for i in range(src.size(0)):
                while neg_dst[i].eq(dst[i]).any():
                    neg_dst[i] = torch.randint(0, num_nodes, (num_neg_samples,), device=device)
            
            # è´Ÿæ ·æœ¬åˆ†æ•°
            src_emb_expanded = embeddings_norm[src].unsqueeze(1)  # [batch, 1, dim]
            neg_emb = embeddings_norm[neg_dst]  # [batch, num_neg_samples, dim]
            neg_scores = (src_emb_expanded * neg_emb).sum(dim=2) / temperature  # [batch, num_neg_samples]
            
            # InfoNCEæŸå¤±
            all_scores = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)  # [batch, 1+num_neg_samples]
            targets = torch.zeros(all_scores.size(0), dtype=torch.long, device=device)
            relation_loss = F.cross_entropy(all_scores, targets)
            
            total_loss += relation_loss
        
        if num_valid_relations == 0:
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        return total_loss / num_valid_relations
    
    def compute_semi_supervised_loss(self, logits, embeddings, labels, mask, edge_indices, alpha=0.5):
        """
        è®¡ç®—åŠç›‘ç£æŸå¤±
        Args:
            logits: [num_nodes, num_classes] é¢„æµ‹logits
            embeddings: [num_nodes, embed_dim] èŠ‚ç‚¹åµŒå…¥
            labels: [num_nodes] æ ‡ç­¾
            mask: [num_nodes] è®­ç»ƒæ©ç 
            edge_indices: list of edge indices
            alpha: ç›‘ç£/æ— ç›‘ç£æŸå¤±å¹³è¡¡å‚æ•°
        """
        # ç›‘ç£æŸå¤±
        sup_loss = F.cross_entropy(logits[mask], labels[mask])
        
        # æ— ç›‘ç£æŸå¤±
        if alpha < 1.0:
            unsup_loss = self.compute_contrastive_loss(embeddings, edge_indices)
            # è‡ªé€‚åº”ç¼©æ”¾ï¼Œé˜²æ­¢æ— ç›‘ç£æŸå¤±è¿‡å¤§
            with torch.no_grad():
                if unsup_loss.item() > sup_loss.item() * 3:
                    scale_factor = sup_loss.item() / unsup_loss.item()
                    unsup_loss = unsup_loss * scale_factor
        else:
            unsup_loss = torch.tensor(0.0, device=sup_loss.device, requires_grad=True)
        
        # ç»„åˆæŸå¤±
        if alpha >= 1.0:
            total_loss = sup_loss
        elif alpha <= 0.0:
            total_loss = unsup_loss
        else:
            total_loss = alpha * sup_loss + (1 - alpha) * unsup_loss
        
        return total_loss, sup_loss, unsup_loss


class SemiSupervisedGraphConvLayer(nn.Module):
    """åŠç›‘ç£å›¾å·ç§¯å±‚"""
    def __init__(self, input_dim, output_dim, dropout=0.5):
        super(SemiSupervisedGraphConvLayer, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, features, edge_index, edge_weight=None):
        num_nodes = features.size(0)
        device = features.device
        
        if edge_index.size(1) == 0:
            return self.dropout(self.linear(features))
        
        # æ¶ˆæ¯ä¼ é€’
        src_features = features[edge_index[0]]
        
        if edge_weight is not None:
            weighted_messages = src_features * edge_weight.unsqueeze(-1)
        else:
            weighted_messages = src_features
        
        # èšåˆåˆ°ç›®æ ‡èŠ‚ç‚¹
        dst_indices = edge_index[1]
        aggregated = torch.zeros(num_nodes, features.size(1), device=device)
        
        for i in range(features.size(1)):
            aggregated[:, i].scatter_add_(0, dst_indices, weighted_messages[:, i])
        
        # çº¿æ€§å˜æ¢
        output = self.linear(aggregated + features)
        return self.dropout(output)


class SemiSupervisedGCN(nn.Module, SemiSupervisedMixin):
    """åŠç›‘ç£GCN"""
    def __init__(self, input_dim, hidden_dim, output_dim, num_classes=2, 
                 num_layers=2, dropout=0.5, alpha=0.5):
        super(SemiSupervisedGCN, self).__init__()
        self.num_layers = num_layers
        self.alpha = alpha
        
        # GCNå±‚
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            if i == 0:
                self.layers.append(SemiSupervisedGraphConvLayer(input_dim, hidden_dim, dropout))
            else:
                self.layers.append(SemiSupervisedGraphConvLayer(hidden_dim, hidden_dim, dropout))
        
        # åµŒå…¥æŠ•å½±å±‚
        self.embedding_projection = nn.Linear(hidden_dim, hidden_dim)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, features, edge_indices, edge_weights, return_embeddings=False):
        x = features
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå…³ç³»å›¾
        if len(edge_indices) > 0:
            edge_index = edge_indices[0]
            edge_weight = edge_weights[0] if len(edge_weights) > 0 else None
        else:
            edge_index = torch.zeros((2, 0), dtype=torch.long, device=features.device)
            edge_weight = None
        
        # é€šè¿‡GCNå±‚
        for layer in self.layers:
            x = F.relu(layer(x, edge_index, edge_weight))
        
        # åµŒå…¥æŠ•å½±
        embeddings = self.embedding_projection(x)
        
        # åˆ†ç±»
        logits = self.classifier(x)
        
        if return_embeddings:
            return logits, embeddings
        return logits


class SemiSupervisedMultiRelationGCN(nn.Module, SemiSupervisedMixin):
    """åŠç›‘ç£å¤šå…³ç³»GCN"""
    def __init__(self, input_dim, hidden_dim, output_dim, num_relations, 
                 num_classes=2, num_layers=2, dropout=0.5, alpha=0.5):
        super(SemiSupervisedMultiRelationGCN, self).__init__()
        self.num_layers = num_layers
        self.num_relations = num_relations
        self.alpha = alpha
        
        # æ¯ä¸ªå…³ç³»çš„GCNå±‚
        self.relation_layers = nn.ModuleList()
        for i in range(num_relations):
            layers = nn.ModuleList()
            for j in range(num_layers):
                if j == 0:
                    layers.append(SemiSupervisedGraphConvLayer(input_dim, hidden_dim, dropout))
                else:
                    layers.append(SemiSupervisedGraphConvLayer(hidden_dim, hidden_dim, dropout))
            self.relation_layers.append(layers)
        
        # å…³ç³»èåˆ
        self.relation_fusion = nn.Linear(hidden_dim * num_relations, hidden_dim)
        
        # åµŒå…¥æŠ•å½±å±‚
        self.embedding_projection = nn.Linear(hidden_dim, hidden_dim)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, features, edge_indices, edge_weights, return_embeddings=False):
        relation_outputs = []
        
        for i in range(min(len(edge_indices), self.num_relations)):
            x = features
            edge_index = edge_indices[i]
            edge_weight = edge_weights[i] if i < len(edge_weights) else None
            
            # é€šè¿‡å…³ç³»ç‰¹å®šçš„GCNå±‚
            for layer in self.relation_layers[i]:
                x = F.relu(layer(x, edge_index, edge_weight))
            
            relation_outputs.append(x)
        
        # å¡«å……ç¼ºå¤±çš„å…³ç³»
        while len(relation_outputs) < self.num_relations:
            relation_outputs.append(torch.zeros_like(relation_outputs[0]))
        
        # èåˆæ‰€æœ‰å…³ç³»
        combined = torch.cat(relation_outputs, dim=-1)
        fused = F.relu(self.relation_fusion(combined))
        
        # åµŒå…¥æŠ•å½±
        embeddings = self.embedding_projection(fused)
        
        # åˆ†ç±»
        logits = self.classifier(fused)
        
        if return_embeddings:
            return logits, embeddings
        return logits


class SemiSupervisedSAGELayer(nn.Module):
    """åŠç›‘ç£GraphSAGEå±‚"""
    def __init__(self, input_dim, output_dim, dropout=0.5, aggregator='mean'):
        super(SemiSupervisedSAGELayer, self).__init__()
        self.aggregator = aggregator
        self.linear = nn.Linear(input_dim * 2, output_dim)  # self + neighbor
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, features, edge_index):
        num_nodes = features.size(0)
        device = features.device
        
        if edge_index.size(1) == 0:
            doubled_features = torch.cat([features, features], dim=-1)
            return self.dropout(self.linear(doubled_features))
        
        # é‚»å±…èšåˆ
        src_features = features[edge_index[0]]
        dst_indices = edge_index[1]
        
        # èšåˆé‚»å±…ç‰¹å¾
        neighbor_features = torch.zeros_like(features)
        if self.aggregator == 'mean':
            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„åº¦
            degree = torch.zeros(num_nodes, device=device)
            degree.scatter_add_(0, dst_indices, torch.ones_like(dst_indices, dtype=torch.float))
            degree = torch.clamp(degree, min=1)
            
            # èšåˆ
            for i in range(features.size(1)):
                neighbor_features[:, i].scatter_add_(0, dst_indices, src_features[:, i])
            neighbor_features = neighbor_features / degree.unsqueeze(-1)
        
        # è¿æ¥è‡ªèº«å’Œé‚»å±…ç‰¹å¾
        combined_features = torch.cat([features, neighbor_features], dim=-1)
        output = self.linear(combined_features)
        
        return self.dropout(output)


class SemiSupervisedGraphSAGE(nn.Module, SemiSupervisedMixin):
    """åŠç›‘ç£GraphSAGE"""
    def __init__(self, input_dim, hidden_dim, output_dim, num_classes=2, 
                 num_layers=2, dropout=0.5, aggregator='mean', alpha=0.5):
        super(SemiSupervisedGraphSAGE, self).__init__()
        self.num_layers = num_layers
        self.aggregator = aggregator
        self.alpha = alpha
        
        # SAGEå±‚
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            if i == 0:
                self.layers.append(SemiSupervisedSAGELayer(input_dim, hidden_dim, dropout, aggregator))
            else:
                self.layers.append(SemiSupervisedSAGELayer(hidden_dim, hidden_dim, dropout, aggregator))
        
        # åµŒå…¥æŠ•å½±å±‚
        self.embedding_projection = nn.Linear(hidden_dim, hidden_dim)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, features, edge_indices, edge_weights, return_embeddings=False):
        x = features
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå…³ç³»å›¾
        if len(edge_indices) > 0:
            edge_index = edge_indices[0]
        else:
            edge_index = torch.zeros((2, 0), dtype=torch.long, device=features.device)
        
        # é€šè¿‡SAGEå±‚
        for layer in self.layers:
            x = F.relu(layer(x, edge_index))
        
        # åµŒå…¥æŠ•å½±
        embeddings = self.embedding_projection(x)
        
        # åˆ†ç±»
        logits = self.classifier(x)
        
        if return_embeddings:
            return logits, embeddings
        return logits


class SemiSupervisedGraphAttentionLayer(nn.Module):
    """åŠç›‘ç£å›¾æ³¨æ„åŠ›å±‚"""
    def __init__(self, input_dim, output_dim, num_heads=1, dropout=0.5):
        super(SemiSupervisedGraphAttentionLayer, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_heads = num_heads
        self.head_dim = output_dim // num_heads
        
        self.linear = nn.Linear(input_dim, output_dim)
        self.attention = nn.Linear(output_dim * 2, num_heads)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, features, edge_index):
        num_nodes = features.size(0)
        device = features.device
        
        if edge_index.size(1) == 0:
            return self.dropout(self.linear(features))
        
        # çº¿æ€§å˜æ¢
        h = self.linear(features)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        src_h = h[edge_index[0]]
        dst_h = h[edge_index[1]]
        
        attention_input = torch.cat([src_h, dst_h], dim=-1)
        attention_weights = self.attention(attention_input)
        attention_weights = F.softmax(attention_weights, dim=0)
        
        # æ¶ˆæ¯ä¼ é€’
        messages = src_h.unsqueeze(1) * attention_weights.unsqueeze(-1)
        
        # èšåˆ
        output = torch.zeros(num_nodes, self.output_dim, device=device)
        for head in range(self.num_heads):
            head_messages = messages[:, head, :]
            head_output = torch.zeros(num_nodes, self.head_dim, device=device)
            
            for i in range(self.head_dim):
                head_output[:, i].scatter_add_(0, edge_index[1], head_messages[:, i])
            
            start_idx = head * self.head_dim
            end_idx = start_idx + self.head_dim
            output[:, start_idx:end_idx] = head_output
        
        return self.dropout(output + h)


class SemiSupervisedGAT(nn.Module, SemiSupervisedMixin):
    """åŠç›‘ç£å›¾æ³¨æ„åŠ›ç½‘ç»œ"""
    def __init__(self, input_dim, hidden_dim, output_dim, num_classes=2, 
                 num_layers=2, num_heads=2, dropout=0.5, alpha=0.5):
        super(SemiSupervisedGAT, self).__init__()
        self.num_layers = num_layers
        self.alpha = alpha
        
        # GATå±‚
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            if i == 0:
                self.layers.append(SemiSupervisedGraphAttentionLayer(input_dim, hidden_dim, num_heads, dropout))
            else:
                self.layers.append(SemiSupervisedGraphAttentionLayer(hidden_dim, hidden_dim, num_heads, dropout))
        
        # åµŒå…¥æŠ•å½±å±‚
        self.embedding_projection = nn.Linear(hidden_dim, hidden_dim)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, features, edge_indices, edge_weights, return_embeddings=False):
        x = features
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå…³ç³»å›¾
        if len(edge_indices) > 0:
            edge_index = edge_indices[0]
        else:
            edge_index = torch.zeros((2, 0), dtype=torch.long, device=features.device)
        
        # é€šè¿‡GATå±‚
        for layer in self.layers:
            x = F.relu(layer(x, edge_index))
        
        # åµŒå…¥æŠ•å½±
        embeddings = self.embedding_projection(x)
        
        # åˆ†ç±»
        logits = self.classifier(x)
        
        if return_embeddings:
            return logits, embeddings
        return logits


class SemiSupervisedMLP(nn.Module, SemiSupervisedMixin):
    """åŠç›‘ç£MLP - ä½¿ç”¨ç‰¹å¾é‡æ„ä½œä¸ºæ— ç›‘ç£ä»»åŠ¡"""
    def __init__(self, input_dim, hidden_dim, output_dim, num_classes=2, 
                 num_layers=2, dropout=0.5, alpha=0.5):
        super(SemiSupervisedMLP, self).__init__()
        self.alpha = alpha
        self.input_dim = input_dim
        
        # ä¸»ç½‘ç»œ
        layers = []
        current_dim = input_dim
        
        for i in range(num_layers):
            layers.append(nn.Linear(current_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            current_dim = hidden_dim
        
        self.encoder = nn.Sequential(*layers)
        
        # åµŒå…¥æŠ•å½±å±‚
        self.embedding_projection = nn.Linear(hidden_dim, hidden_dim)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Linear(hidden_dim, num_classes)
        
        # ç‰¹å¾é‡æ„å™¨ï¼ˆç”¨äºæ— ç›‘ç£å­¦ä¹ ï¼‰
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, input_dim)
        )
        
    def forward(self, features, edge_indices=None, edge_weights=None, return_embeddings=False):
        # ç¼–ç 
        encoded = self.encoder(features)
        
        # åµŒå…¥æŠ•å½±
        embeddings = self.embedding_projection(encoded)
        
        # åˆ†ç±»
        logits = self.classifier(encoded)
        
        if return_embeddings:
            return logits, embeddings, encoded
        return logits
    
    def compute_reconstruction_loss(self, features, encoded):
        """è®¡ç®—é‡æ„æŸå¤±ä½œä¸ºæ— ç›‘ç£æŸå¤±"""
        reconstructed = self.decoder(encoded)
        return F.mse_loss(reconstructed, features)
    
    def compute_semi_supervised_loss(self, logits, embeddings, labels, mask, edge_indices, alpha=None):
        """é‡å†™MLPçš„åŠç›‘ç£æŸå¤±"""
        if alpha is None:
            alpha = self.alpha
            
        # ç›‘ç£æŸå¤±
        sup_loss = F.cross_entropy(logits[mask], labels[mask])
        
        # æ— ç›‘ç£æŸå¤±ï¼ˆç‰¹å¾é‡æ„ï¼‰
        if alpha < 1.0:
            # è·å–ç¼–ç ç‰¹å¾
            with torch.no_grad():
                _, _, encoded = self.forward(embeddings.detach(), return_embeddings=True)
            unsup_loss = self.compute_reconstruction_loss(embeddings.detach(), encoded)
        else:
            unsup_loss = torch.tensor(0.0, device=sup_loss.device, requires_grad=True)
        
        # ç»„åˆæŸå¤±
        if alpha >= 1.0:
            total_loss = sup_loss
        elif alpha <= 0.0:
            total_loss = unsup_loss
        else:
            total_loss = alpha * sup_loss + (1 - alpha) * unsup_loss
        
        return total_loss, sup_loss, unsup_loss


def train_semi_supervised_baseline(model, features, labels, edge_indices, edge_weights,
                                 train_mask, val_mask, test_mask, args):
    """è®­ç»ƒåŠç›‘ç£åŸºçº¿æ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() and args.cuda else 'cpu')
    model = model.to(device)
    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    best_val_f1 = 0
    best_test_metrics = {}
    patience = 0
    max_patience = 30
    
    print(f"Training with Î±={model.alpha if hasattr(model, 'alpha') else 'N/A'}")
    
    for epoch in range(args.epochs):
        model.train()
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        if isinstance(model, SemiSupervisedMLP):
            logits, embeddings, encoded = model(features, edge_indices, edge_weights, return_embeddings=True)
            # MLPä½¿ç”¨ç‰¹å¾ä½œä¸º"åµŒå…¥"è¿›è¡Œé‡æ„
            total_loss, sup_loss, unsup_loss = model.compute_semi_supervised_loss(
                logits, features, labels, train_mask, edge_indices
            )
        else:
            logits, embeddings = model(features, edge_indices, edge_weights, return_embeddings=True)
            total_loss, sup_loss, unsup_loss = model.compute_semi_supervised_loss(
                logits, embeddings, labels, train_mask, edge_indices
            )
        
        # åå‘ä¼ æ’­
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # è¯„ä¼°
        if epoch % args.eval_every == 0:
            val_metrics = evaluate_semi_supervised_model(model, features, labels, edge_indices, 
                                                       edge_weights, val_mask)
            test_metrics = evaluate_semi_supervised_model(model, features, labels, edge_indices, 
                                                        edge_weights, test_mask)
            
            if epoch % (args.eval_every * 2) == 0:
                print(f"Epoch {epoch:03d} | Total: {total_loss:.4f} "
                      f"(Sup: {sup_loss:.4f}, Unsup: {unsup_loss:.4f}) | "
                      f"Val F1: {val_metrics.get('f1', 0):.4f} | "
                      f"Test F1: {test_metrics.get('f1', 0):.4f}")
            
            # æ—©åœæœºåˆ¶
            if val_metrics.get('f1', 0) > best_val_f1:
                best_val_f1 = val_metrics.get('f1', 0)
                best_test_metrics = test_metrics
                patience = 0
            else:
                patience += 1
                if patience >= max_patience:
                    break
    
    return best_test_metrics


def evaluate_semi_supervised_model(model, features, labels, edge_indices, edge_weights, mask):
    """è¯„ä¼°åŠç›‘ç£æ¨¡å‹"""
    model.eval()
    with torch.no_grad():
        logits = model(features, edge_indices, edge_weights)
        probs = F.softmax(logits, dim=1)
        preds = torch.argmax(logits, dim=1)
        
        # åªè¯„ä¼°æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹
        valid_mask = (labels >= 0) & mask
        if valid_mask.sum() == 0:
            return {}
        
        true_labels = labels[valid_mask].cpu().numpy()
        pred_labels = preds[valid_mask].cpu().numpy()
        pred_probs = probs[valid_mask, 1].cpu().numpy()
        
        metrics = {
            'accuracy': accuracy_score(true_labels, pred_labels),
            'precision': precision_score(true_labels, pred_labels, average='macro', zero_division=0),
            'recall': recall_score(true_labels, pred_labels, average='macro', zero_division=0),
            'f1': f1_score(true_labels, pred_labels, average='macro', zero_division=0),
        }
        
        # è®¡ç®—AUCï¼ˆå¦‚æœæœ‰æ­£è´Ÿæ ·æœ¬ï¼‰
        if len(np.unique(true_labels)) > 1:
            metrics['auc'] = roc_auc_score(true_labels, pred_probs)
        
        return metrics


def run_semi_supervised_comparison(data_dir, args):
    """è¿è¡ŒåŠç›‘ç£æ¨¡å‹å¯¹æ¯”å®éªŒ"""
    print("\n" + "="*80)
    print("SEMI-SUPERVISED MODEL COMPARISON STUDY")
    print("="*80)
    
    set_seed(args.seed)
    
    # åŠ è½½æ•°æ®
    features, labels, adj_matrices = load_data(data_dir)
    attr_edge_index, attr_edge_weight = construct_attribute_graph(
        features, k_neighbors=args.k_neighbors
    )
    
    # è½¬æ¢ä¸ºedge_indexæ ¼å¼
    edge_indices = []
    edge_weights = []
    for adj in adj_matrices:
        edge_index, edge_weight = sparse_to_edge_index(adj)
        edge_indices.append(edge_index)
        edge_weights.append(edge_weight)
    edge_indices.append(attr_edge_index)
    edge_weights.append(attr_edge_weight)
    
    # æ•°æ®é¢„å¤„ç†
    features = torch.FloatTensor(features)
    labels = torch.LongTensor(labels)
    
    # åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ©ç 
    valid_nodes = (labels >= 0).numpy()
    valid_indices = np.where(valid_nodes)[0]
    train_idx, temp_idx = train_test_split(valid_indices, test_size=0.4, random_state=args.seed)
    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=args.seed)
    
    train_mask = torch.zeros(len(labels), dtype=torch.bool)
    val_mask = torch.zeros(len(labels), dtype=torch.bool)
    test_mask = torch.zeros(len(labels), dtype=torch.bool)
    train_mask[train_idx] = True
    val_mask[val_idx] = True
    test_mask[test_idx] = True
    
    # ç§»åŠ¨åˆ°GPU
    device = torch.device('cuda' if torch.cuda.is_available() and args.cuda else 'cpu')
    features = features.to(device)
    labels = labels.to(device)
    train_mask = train_mask.to(device)
    val_mask = val_mask.to(device)
    test_mask = test_mask.to(device)
    edge_indices = [ei.to(device) for ei in edge_indices]
    edge_weights = [ew.to(device) for ew in edge_weights]
    
    results = {}
    
    # æµ‹è¯•ä¸åŒçš„Î±å€¼
    alpha_values = [0.0, 0.3, 0.5, 0.7, 1.0] if args.test_alpha_values else [args.alpha]
    
    for alpha in alpha_values:
        print(f"\nğŸ” Testing models with Î±={alpha}")
        alpha_results = {}
        
        # 1. åŠç›‘ç£MLP
        print(f"\nğŸ” Testing Semi-Supervised MLP (Î±={alpha})...")
        mlp_model = SemiSupervisedMLP(
            input_dim=features.shape[1],
            hidden_dim=args.hidden_dim,
            output_dim=args.hidden_dim,
            num_classes=2,
            num_layers=args.num_layers,
            dropout=args.dropout,
            alpha=alpha
        )
        mlp_results = train_semi_supervised_baseline(
            mlp_model, features, labels, edge_indices, edge_weights,
            train_mask, val_mask, test_mask, args
        )
        alpha_results['Semi_MLP'] = mlp_results
        
        # 2. åŠç›‘ç£GCN
        print(f"\nğŸ” Testing Semi-Supervised GCN (Î±={alpha})...")
        gcn_model = SemiSupervisedGCN(
            input_dim=features.shape[1],
            hidden_dim=args.hidden_dim,
            output_dim=args.hidden_dim,
            num_classes=2,
            num_layers=args.num_layers,
            dropout=args.dropout,
            alpha=alpha
        )
        gcn_results = train_semi_supervised_baseline(
            gcn_model, features, labels, edge_indices, edge_weights,
            train_mask, val_mask, test_mask, args
        )
        alpha_results['Semi_GCN'] = gcn_results
        
        # 3. åŠç›‘ç£å¤šå…³ç³»GCN
        print(f"\nğŸ” Testing Semi-Supervised Multi-Relation GCN (Î±={alpha})...")
        mr_gcn_model = SemiSupervisedMultiRelationGCN(
            input_dim=features.shape[1],
            hidden_dim=args.hidden_dim,
            output_dim=args.hidden_dim,
            num_relations=len(adj_matrices),
            num_classes=2,
            num_layers=args.num_layers,
            dropout=args.dropout,
            alpha=alpha
        )
        mr_gcn_results = train_semi_supervised_baseline(
            mr_gcn_model, features, labels, edge_indices, edge_weights,
            train_mask, val_mask, test_mask, args
        )
        alpha_results['Semi_MultiRelation_GCN'] = mr_gcn_results
        
        # 4. åŠç›‘ç£GraphSAGE
        print(f"\nğŸ” Testing Semi-Supervised GraphSAGE (Î±={alpha})...")
        sage_model = SemiSupervisedGraphSAGE(
            input_dim=features.shape[1],
            hidden_dim=args.hidden_dim,
            output_dim=args.hidden_dim,
            num_classes=2,
            num_layers=args.num_layers,
            dropout=args.dropout,
            alpha=alpha
        )
        sage_results = train_semi_supervised_baseline(
            sage_model, features, labels, edge_indices, edge_weights,
            train_mask, val_mask, test_mask, args
        )
        alpha_results['Semi_GraphSAGE'] = sage_results
        
        # 5. åŠç›‘ç£GAT
        print(f"\nğŸ” Testing Semi-Supervised GAT (Î±={alpha})...")
        gat_model = SemiSupervisedGAT(
            input_dim=features.shape[1],
            hidden_dim=args.hidden_dim,
            output_dim=args.hidden_dim,
            num_classes=2,
            num_layers=args.num_layers,
            num_heads=2,
            dropout=args.dropout,
            alpha=alpha
        )
        gat_results = train_semi_supervised_baseline(
            gat_model, features, labels, edge_indices, edge_weights,
            train_mask, val_mask, test_mask, args
        )
        alpha_results['Semi_GAT'] = gat_results
        
        # 6. Enhanced CARE-GNNï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰
        if alpha == args.alpha:  # åªåœ¨é»˜è®¤Î±å€¼æ—¶æµ‹è¯•
            print(f"\nğŸ” Testing Enhanced CARE-GNN (Î±={alpha})...")
            care_gnn_model = EnhancedCAREGNN(
                input_dim=features.shape[1],
                hidden_dim=args.hidden_dim,
                output_dim=args.hidden_dim,
                num_relations=len(adj_matrices),
                num_views=len(edge_indices),
                num_classes=2,
                num_layers=args.num_layers,
                dropout=args.dropout,
                alpha=alpha
            )
            care_gnn_results = train_enhanced_care_gnn_for_comparison(
                care_gnn_model, features, labels, edge_indices, edge_weights,
                train_mask, val_mask, test_mask, args
            )
            alpha_results['Enhanced_CARE_GNN'] = care_gnn_results
        
        results[f'alpha_{alpha}'] = alpha_results
    
    # è¾“å‡ºç»“æœ
    print_semi_supervised_results(results, args)
    
    return results


def train_enhanced_care_gnn_for_comparison(model, features, labels, edge_indices, edge_weights,
                                         train_mask, val_mask, test_mask, args):
    """ä¸ºå¯¹æ¯”å®éªŒè®­ç»ƒEnhanced CARE-GNN"""
    device = torch.device('cuda' if torch.cuda.is_available() and args.cuda else 'cpu')
    model = model.to(device)
    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    best_val_f1 = 0
    best_test_metrics = {}
    patience = 0
    max_patience = 30
    
    for epoch in range(args.epochs):
        model.train()
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        final_logits, aux_predictions, embeddings, _ = model(
            features, edge_indices, edge_weights, return_embeddings=True
        )
        
        # è®¡ç®—æŸå¤±
        total_loss, sup_loss, graph_loss = model.compute_loss(
            final_logits, aux_predictions, labels, train_mask,
            embeddings, edge_indices
        )
        
        # åå‘ä¼ æ’­
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # è¯„ä¼°
        if epoch % args.eval_every == 0:
            val_metrics = evaluate_model(model, features, labels, edge_indices, edge_weights, val_mask)
            test_metrics = evaluate_model(model, features, labels, edge_indices, edge_weights, test_mask)
            
            if epoch % (args.eval_every * 2) == 0:
                print(f"Epoch {epoch:03d} | Total: {total_loss:.4f} "
                      f"(Sup: {sup_loss:.4f}, Graph: {graph_loss:.4f}) | "
                      f"Val F1: {val_metrics.get('f1', 0):.4f} | "
                      f"Test F1: {test_metrics.get('f1', 0):.4f}")
            
            # æ—©åœæœºåˆ¶
            if val_metrics.get('f1', 0) > best_val_f1:
                best_val_f1 = val_metrics.get('f1', 0)
                best_test_metrics = test_metrics
                patience = 0
            else:
                patience += 1
                if patience >= max_patience:
                    break
    
    return best_test_metrics


def print_semi_supervised_results(results, args):
    """æ‰“å°åŠç›‘ç£å®éªŒç»“æœ"""
    print("\n" + "="*100)
    print("SEMI-SUPERVISED MODEL COMPARISON RESULTS")
    print("="*100)
    
    if args.test_alpha_values:
        # Î±å€¼åˆ†æ
        print(f"\nğŸ“Š Alpha Value Analysis:")
        print(f"{'Model':<25} " + " ".join([f"Î±={Î±:<6}" for Î± in [0.0, 0.3, 0.5, 0.7, 1.0]]))
        print("-" * 100)
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹åç§°
        all_models = set()
        for alpha_key, alpha_results in results.items():
            all_models.update(alpha_results.keys())
        
        for model_name in sorted(all_models):
            print(f"{model_name:<25} ", end="")
            for alpha in [0.0, 0.3, 0.5, 0.7, 1.0]:
                alpha_key = f"alpha_{alpha}"
                if alpha_key in results and model_name in results[alpha_key]:
                    f1_score = results[alpha_key][model_name].get('f1', 0)
                    print(f"{f1_score:<8.4f}", end="")
                else:
                    print(f"{'N/A':<8}", end="")
            print()
    
    # æœ€ä½³ç»“æœå¯¹æ¯”ï¼ˆä½¿ç”¨é»˜è®¤Î±å€¼ï¼‰
    default_alpha_key = f"alpha_{args.alpha}"
    if default_alpha_key in results:
        print(f"\nğŸ“ˆ Best Results Comparison (Î±={args.alpha}):")
        print(f"{'Model':<25} {'F1':<8} {'AUC':<8} {'Accuracy':<10} {'Precision':<10} {'Recall':<8}")
        print("-" * 80)
        
        alpha_results = results[default_alpha_key]
        sorted_results = sorted(alpha_results.items(), key=lambda x: x[1].get('f1', 0), reverse=True)
        
        for model_name, metrics in sorted_results:
            print(f"{model_name:<25} "
                  f"{metrics.get('f1', 0):<8.4f} "
                  f"{metrics.get('auc', 0):<8.4f} "
                  f"{metrics.get('accuracy', 0):<10.4f} "
                  f"{metrics.get('precision', 0):<10.4f} "
                  f"{metrics.get('recall', 0):<8.4f}")
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        if 'Enhanced_CARE_GNN' in alpha_results:
            care_gnn_f1 = alpha_results['Enhanced_CARE_GNN'].get('f1', 0)
            best_baseline_f1 = 0
            best_baseline_name = ""
            
            for model_name, metrics in alpha_results.items():
                if model_name != 'Enhanced_CARE_GNN' and metrics.get('f1', 0) > best_baseline_f1:
                    best_baseline_f1 = metrics.get('f1', 0)
                    best_baseline_name = model_name
            
            if best_baseline_f1 > 0:
                improvement = ((care_gnn_f1 - best_baseline_f1) / best_baseline_f1) * 100
                print(f"\nğŸš€ Enhanced CARE-GNN vs Best Semi-Supervised Baseline ({best_baseline_name}):")
                print(f"   Improvement: {improvement:+.2f}% F1 score")
                print(f"   Absolute difference: {care_gnn_f1 - best_baseline_f1:+.4f} F1")


def analyze_alpha_sensitivity(results):
    """åˆ†æÎ±å‚æ•°æ•æ„Ÿæ€§"""
    print("\n" + "="*80)
    print("ALPHA PARAMETER SENSITIVITY ANALYSIS")
    print("="*80)
    
    # æ”¶é›†æ‰€æœ‰Î±å€¼ä¸‹çš„ç»“æœ
    alpha_analysis = {}
    
    for alpha_key, alpha_results in results.items():
        alpha_val = float(alpha_key.split('_')[1])
        
        for model_name, metrics in alpha_results.items():
            if model_name not in alpha_analysis:
                alpha_analysis[model_name] = {'alphas': [], 'f1_scores': []}
            
            alpha_analysis[model_name]['alphas'].append(alpha_val)
            alpha_analysis[model_name]['f1_scores'].append(metrics.get('f1', 0))
    
    # åˆ†ææ¯ä¸ªæ¨¡å‹çš„æœ€ä½³Î±å€¼
    print(f"{'Model':<25} {'Best Î±':<8} {'Best F1':<10} {'Î± Range':<15} {'Stability':<10}")
    print("-" * 80)
    
    for model_name, data in alpha_analysis.items():
        alphas = np.array(data['alphas'])
        f1_scores = np.array(data['f1_scores'])
        
        # æ’åº
        sorted_indices = np.argsort(alphas)
        alphas = alphas[sorted_indices]
        f1_scores = f1_scores[sorted_indices]
        
        # æ‰¾åˆ°æœ€ä½³Î±
        best_idx = np.argmax(f1_scores)
        best_alpha = alphas[best_idx]
        best_f1 = f1_scores[best_idx]
        
        # è®¡ç®—ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®ï¼‰
        stability = np.std(f1_scores)
        
        # Î±å€¼èŒƒå›´ï¼ˆF1 > 0.9 * best_f1çš„Î±èŒƒå›´ï¼‰
        good_indices = f1_scores >= 0.9 * best_f1
        if np.any(good_indices):
            alpha_range = f"{alphas[good_indices].min():.1f}-{alphas[good_indices].max():.1f}"
        else:
            alpha_range = f"{best_alpha:.1f}"
        
        print(f"{model_name:<25} "
              f"{best_alpha:<8.1f} "
              f"{best_f1:<10.4f} "
              f"{alpha_range:<15} "
              f"{stability:<10.4f}")


def create_alpha_sensitivity_plot(results, save_path='semi_supervised_results/'):
    """åˆ›å»ºÎ±æ•æ„Ÿæ€§åˆ†æå›¾"""
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        os.makedirs(save_path, exist_ok=True)
        
        # æ”¶é›†æ•°æ®
        plot_data = {}
        for alpha_key, alpha_results in results.items():
            alpha_val = float(alpha_key.split('_')[1])
            
            for model_name, metrics in alpha_results.items():
                if model_name not in plot_data:
                    plot_data[model_name] = {'alphas': [], 'f1_scores': []}
                
                plot_data[model_name]['alphas'].append(alpha_val)
                plot_data[model_name]['f1_scores'].append(metrics.get('f1', 0))
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 8))
        
        for model_name, data in plot_data.items():
            alphas = np.array(data['alphas'])
            f1_scores = np.array(data['f1_scores'])
            
            # æ’åº
            sorted_indices = np.argsort(alphas)
            alphas = alphas[sorted_indices]
            f1_scores = f1_scores[sorted_indices]
            
            # ç»˜åˆ¶çº¿æ¡
            linestyle = '-' if 'CARE_GNN' in model_name else '--'
            linewidth = 3 if 'CARE_GNN' in model_name else 1.5
            
            plt.plot(alphas, f1_scores, 'o-', label=model_name, 
                    linestyle=linestyle, linewidth=linewidth, markersize=6)
        
        plt.xlabel('Alpha (Î±)', fontsize=14)
        plt.ylabel('F1 Score', fontsize=14)
        plt.title('Alpha Parameter Sensitivity Analysis', fontsize=16)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(os.path.join(save_path, 'alpha_sensitivity.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Alpha sensitivity plot saved in '{save_path}'")
        
    except ImportError:
        print("âš ï¸  Matplotlib/Seaborn not available, skipping alpha sensitivity plot")


def save_semi_supervised_results(results, save_path='semi_supervised_results/'):
    """ä¿å­˜åŠç›‘ç£å®éªŒç»“æœ"""
    os.makedirs(save_path, exist_ok=True)
    
    # ä¿å­˜åŸå§‹ç»“æœ
    serializable_results = {}
    for alpha_key, alpha_results in results.items():
        serializable_results[alpha_key] = {}
        for model, metrics in alpha_results.items():
            serializable_results[alpha_key][model] = {
                k: float(v) if isinstance(v, (np.float32, np.float64)) else v
                for k, v in metrics.items()
            }
    
    with open(os.path.join(save_path, 'semi_supervised_results.json'), 'w') as f:
        json.dump(serializable_results, f, indent=2)
    
    # åˆ›å»ºCSVæ–‡ä»¶
    import pandas as pd
    
    # æ‰å¹³åŒ–ç»“æœç”¨äºCSV
    csv_data = []
    for alpha_key, alpha_results in results.items():
        alpha_val = float(alpha_key.split('_')[1])
        for model, metrics in alpha_results.items():
            row = {'Alpha': alpha_val, 'Model': model}
            row.update(metrics)
            csv_data.append(row)
    
    df = pd.DataFrame(csv_data)
    df.to_csv(os.path.join(save_path, 'semi_supervised_results.csv'), index=False)
    
    print(f"âœ… Semi-supervised results saved in '{save_path}'")


def main():
    parser = argparse.ArgumentParser(description='Semi-Supervised GNN Model Comparison')
    parser.add_argument('--data_dir', type=str, default='small_multirel_dataset',
                        help='Directory containing the dataset')
    parser.add_argument('--hidden_dim', type=int, default=64,
                        help='Hidden dimension size')
    parser.add_argument('--num_layers', type=int, default=2,
                        help='Number of GNN layers')
    parser.add_argument('--dropout', type=float, default=0.5,
                        help='Dropout rate')
    parser.add_argument('--lr', type=float, default=0.01,
                        help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=5e-4,
                        help='Weight decay')
    parser.add_argument('--epochs', type=int, default=150,
                        help='Number of training epochs')
    parser.add_argument('--eval_every', type=int, default=10,
                        help='Evaluate every N epochs')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed')
    parser.add_argument('--cuda', action='store_true',
                        help='Use CUDA if available')
    parser.add_argument('--alpha', type=float, default=0.5,
                        help='Default alpha value for supervised/unsupervised balance')
    parser.add_argument('--k_neighbors', type=int, default=10,
                        help='Number of neighbors for attribute graph construction')
    
    # å®éªŒæ§åˆ¶å‚æ•°
    parser.add_argument('--run_semi_comparison', action='store_true',
                        help='Run semi-supervised model comparison')
    parser.add_argument('--test_alpha_values', action='store_true',
                        help='Test multiple alpha values for sensitivity analysis')
    parser.add_argument('--analyze_alpha', action='store_true',
                        help='Analyze alpha parameter sensitivity')
    parser.add_argument('--visualize_alpha', action='store_true',
                        help='Create alpha sensitivity visualization')
    parser.add_argument('--save_results', action='store_true',
                        help='Save detailed results')
    
    args = parser.parse_args()
    
    if args.run_semi_comparison:
        print("ğŸš€ Starting Semi-Supervised Model Comparison...")
        
        # è¿è¡ŒåŠç›‘ç£å¯¹æ¯”å®éªŒ
        results = run_semi_supervised_comparison(args.data_dir, args)
        
        # Î±æ•æ„Ÿæ€§åˆ†æ
        if args.analyze_alpha and args.test_alpha_values:
            analyze_alpha_sensitivity(results)
        
        # åˆ›å»ºÎ±æ•æ„Ÿæ€§å¯è§†åŒ–
        if args.visualize_alpha and args.test_alpha_values:
            create_alpha_sensitivity_plot(results)
        
        # ä¿å­˜ç»“æœ
        if args.save_results:
            save_semi_supervised_results(results)
        
        print(f"\nğŸ‰ Semi-supervised comparison completed!")
        
        # è¾“å‡ºå®éªŒå»ºè®®
        print(f"\nğŸ’¡ Experimental Insights:")
        print(f"   âœ… All baselines now use semi-supervised learning")
        print(f"   âœ… Fair comparison with your Enhanced CARE-GNN")
        print(f"   âœ… Alpha sensitivity analysis reveals optimal hyperparameters")
        print(f"   âœ… Demonstrates the value of your model's innovations beyond semi-supervision")
        
    else:
        print("Use --run_semi_comparison to start the semi-supervised model comparison")
        print("Additional options:")
        print("  --test_alpha_values: Test multiple alpha values (0.0, 0.3, 0.5, 0.7, 1.0)")
        print("  --analyze_alpha: Analyze alpha parameter sensitivity")
        print("  --visualize_alpha: Create alpha sensitivity plots")
        print("  --save_results: Save detailed results")


if __name__ == '__main__':
    main()